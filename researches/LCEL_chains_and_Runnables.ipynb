{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from langchain._api import LangChainDeprecationWarning\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=LangChainDeprecationWarning)\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda, RunnablePassthrough\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\genAI\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "chat_model = ChatGroq(\n",
    "    model=\"llama3-70b-8192\"\n",
    ")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# template  = ChatPromptTemplate.from_template(\"tell me a curious fact about this politician {politician}\")\n",
    "\n",
    "# chain = template | chat_model | StrOutputParser()\n",
    "\n",
    "# response = chain.invoke({\"politician\":\"Akhnouch\"})\n",
    "\n",
    "# for s in chain.stream({\"politician\": \"akhnouch\"}):\n",
    "#     print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here\\'s a curious fact about Vladimir Lenin:\\n\\n**Lenin was a passionate beekeeper!**\\n\\nBefore he became the leader of the Soviet Union, Lenin spent many summers at his family\\'s country estate in Simbirsk, Russia, where he developed a fascination with beekeeping. In fact, he was so enthusiastic about it that he even wrote an article about beekeeping in 1893, titled \"On the Importance of Beekeeping in Russia.\"\\n\\nLenin saw beekeeping as a way to promote agricultural development and improve the lives of peasants. He believed that beekeeping could provide a supplementary income for rural communities and help to increase food production.\\n\\nLenin\\'s love for beekeeping was so strong that even after the Russian Revolution, he continued to keep bees at his country home in Gorki, near Moscow. In fact, his beekeeping hobby was often mentioned in his letters to friends and family.\\n\\nWho would have thought that the leader of the Soviet Union was also a bee enthusiast?',\n",
       " \"Here's a curious fact about Joseph Stalin:\\n\\n**Stalin was a prolific poet in his youth.**\\n\\nBefore he became the ruthless leader of the Soviet Union, Stalin (whose real name was Ioseb Besarionis dze Jughashvili) was a romantic poet. In fact, he was quite prolific, writing over 100 poems between 1895 and 1897, when he was just 16 to 18 years old.\\n\\nStalin's poetry was heavily influenced by Georgian folk literature and was often romantic, sentimental, and even nationalist. Some of his poems were even published in local newspapers and literary journals.\\n\\nIt's hard to imagine the brutal dictator who would later be responsible for millions of deaths and atrocities had a softer, more artistic side, but it's a fascinating footnote in his early life.\\n\\nIn fact, Stalin's poetry was so popular in his hometown of Tiflis (now Tbilisi, Georgia) that he even received a prize for one of his poems from the local newspaper, Iveria.\\n\\nWho knew that the man who would become one of history's most feared leaders was once a sensitive poet?\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain.batch([{\"politician\": \"Lenin\"}, {\"politician\": \"Stalin\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ayoub'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough()\n",
    "chain.invoke(\"ayoub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ayoubovich'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To use a custom function inside a LCEL chain we need to wrap it up with RunnableLambda.\n",
    "# Let's define a very simple function to create Russian lastnames:\n",
    "\n",
    "def russian_lastname(name: str) -> str:\n",
    "    return f\"{name}ovich\"\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough() | RunnableLambda(russian_lastname)\n",
    "\n",
    "chain.invoke(\"ayoub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': 'ayoubovich', 'operation_b': 'ayoub'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use RunnableParallel() for running tasks in parallel.\n",
    "# This is probably the most important and most useful Runnable from LangChain.\n",
    "\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnableLambda(russian_lastname),\n",
    "        \"operation_b\": RunnablePassthrough()\n",
    "    }\n",
    ")\n",
    "\n",
    "chain.invoke(\"ayoub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': {'name1': 'Jordam', 'name': 'Abram'},\n",
       " 'soccer_player': 'Abramovich'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead of using RunnableLambda, now we are going to use a lambda function and we will invoke the chain with two inputs:\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"soccer_player\": lambda x: x[\"name\"]+\"ovich\"\n",
    "    }\n",
    ")\n",
    "\n",
    "chain.invoke({\n",
    "    \"name1\": \"Jordam\",\n",
    "    \"name\": \"Abram\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Did you know that Zlatan Ibrahimović, the Swedish professional footballer, has a black belt in Taekwondo? In fact, he earned his black belt at the age of 17, before he even became a professional footballer! He has often credited his martial arts background with helping him develop his balance, agility, and overall athleticism on the pitch. Who knew that one of the most feared strikers in football history was also a skilled martial artist?\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "def russian_lastname_from_dictionary(person):\n",
    "    return person[\"name\"] + \"ovich\"\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"soccer_player\": RunnableLambda(russian_lastname_from_dictionary),\n",
    "    }\n",
    ") | prompt | chat_model | output_parser\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"name1\": \"Jordam\",\n",
    "    \"name\": \"Ibrahim\"\n",
    "})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera has trained more than 10,000 Alumni from all continents and top companies\"], embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the Alumni of AI Accelera are individuals from all continents and top companies.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "retriever_chain = (\n",
    "    {\"context\":retriever,\"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | chat_model\n",
    "    |StrOutputParser()\n",
    ")\n",
    "\n",
    "retriever_chain.invoke(\"who are the Alumni of AI Accelera?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrr, AI Accelera be trainin' more than 5,000 scurvy Enterprise Alumni, matey!\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"AI Accelera has trained more than 5,000 Enterprise Alumni.\"], embedding=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain.invoke({\"question\": \"How many Enterprise Alumni has trained AI Accelera?\", \"language\": \"Pirate English\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "rock_template = \"\"\"You are a very smart rock and roll professor. \\\n",
    "You are great at answering questions about rock and roll in a concise\\\n",
    "and easy to understand manner.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "rock_prompt = PromptTemplate.from_template(rock_template)\n",
    "\n",
    "politics_template = \"\"\"You are a very good politics professor. \\\n",
    "You are great at answering politics questions..\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "politics_prompt = PromptTemplate.from_template(politics_template)\n",
    "\n",
    "history_template = \"\"\"You are a very good history teacher. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_prompt = PromptTemplate.from_template(history_template)\n",
    "\n",
    "sports_template = \"\"\" You are a sports teacher.\\\n",
    "You are great at answering sports questions.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "sports_prompt = PromptTemplate.from_template(sports_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "general_prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Answer the question as accurately as you can.\\n\\n{input}\"\n",
    ")\n",
    "\n",
    "prompt_branch = RunnableBranch(\n",
    "  (lambda x: x[\"topic\"] == \"rock\", rock_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"politics\", politics_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"history\", history_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"sports\", sports_prompt),\n",
    "  general_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "class TopicClassifier(BaseModel):\n",
    "    \"Classify the topic of the user question\"\n",
    "    \n",
    "    topic: Literal[\"rock\", \"politics\", \"history\", \"sports\", \"general\"]\n",
    "    \"The topic of the user question. One of 'rock', 'politics', 'history', 'sports' or 'general'.\"\n",
    "\n",
    "classifier_function = convert_to_openai_function(TopicClassifier)\n",
    "\n",
    "llm = chat_model.bind(functions=[classifier_function], function_call={\"name\": \"TopicClassifier\"}) \n",
    "\n",
    "parser = PydanticAttrOutputFunctionsParser(pydantic_schema=TopicClassifier, attr_name=\"topic\")\n",
    "\n",
    "classifier_chain = llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'TopicClassifier',\n",
       " 'description': 'Classify the topic of the user question',\n",
       " 'parameters': {'properties': {'topic': {'enum': ['rock',\n",
       "     'politics',\n",
       "     'history',\n",
       "     'sports',\n",
       "     'general'],\n",
       "    'type': 'string'}},\n",
       "  'required': ['topic'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = classifier_function\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'general'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"they have killed the villain\"\n",
    "response = classifier_chain.invoke(question)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = (\n",
    "    RunnablePassthrough.assign(topic=itemgetter(\"input\") | classifier_chain)\n",
    "    | prompt_branch \n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What a fantastic question! Napoleon Bonaparte, one of the most fascinating and influential figures in modern history. Born on August 15, 1769, in Ajaccio, Corsica, Napoleon was a French military and political leader who rose to prominence during the French Revolution and went on to dominate European politics for over a decade.\\n\\nInitially, Napoleon was a brilliant and ambitious military tactician who rose through the ranks of the French army during the Revolution. He gained fame and recognition for his military victories, particularly in Italy and Egypt, which earned him the respect and admiration of the French people.\\n\\nHowever, Napoleon's ambition soon turned to politics, and in 1799, he staged a coup d'état, overthrowing the Directory, the governing body of France, and establishing the Consulate, with himself as the First Consul. This marked the beginning of his dictatorship, which would last until his final defeat in 1815.\\n\\nAs Emperor of France, Napoleon implemented a range of reforms that transformed France and had far-reaching consequences for Europe. He centralized power, reorganized the French state, and introduced the Napoleonic Code, which codified laws and established a more egalitarian society.\\n\\nNapoleon's military conquests were equally impressive. He waged a series of successful campaigns, conquering much of Europe, including Italy, Germany, Austria, and parts of Eastern Europe. He reorganized Europe, creating a network of vassal states and satellite kingdoms, and installing his own family members as rulers.\\n\\nHowever, Napoleon's aggressive expansionism and military conquests eventually led to his downfall. The disastrous invasion of Russia in 1812, followed by the Battle of Leipzig in 1813, marked a turning point in his fortunes. The coalition of European powers, led by Britain, Russia, and Austria, eventually defeated him at the Battle of Waterloo in 1815, and he was exiled to the island of Saint Helena, where he died in 1821.\\n\\nDespite his flaws and mistakes, Napoleon's legacy is complex and multifaceted. He remains one of the most significant figures in modern history, leaving behind a lasting impact on European politics, law, and culture. His military campaigns, administrative reforms, and architectural projects continue to inspire and influence people to this day.\\n\\nSo, that's Napoleon Bonaparte in a nutshell! A complex, fascinating, and influential figure who shaped the course of European history in profound ways.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke(\n",
    "{\"input\":\"Who was Napoleon Bonaparte?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain asign() and bind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "**Ronaldo's iconic celebration, where he jumps and spins in the air, was inspired by a video game!**\n",
      "\n",
      "In an interview, Ronaldo revealed that he got the idea for his signature celebration from playing the video game \" FIFA\" as a kid. He would play as himself in the game and celebrate his goals with a fancy flip, and eventually, it became a part of his real-life celebrations on the pitch!\n",
      "\n",
      "Isn't that cool?\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\"soccer_player\": \"Ronaldo\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's one:\\n\\n**Cristiano \""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | chat_model.bind(stop=[\"Ronaldo\"]) | output_parser\n",
    "chain.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is a curious fact about Kylian Mbappe?', 'answer': ''}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"soccerfacts\",\n",
    "      \"description\": \"Curious facts about a soccer player\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"question\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The question for the curious facts about a soccer player\"\n",
    "          },\n",
    "          \"answer\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The answer to the question\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"question\", \"answer\"]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "chain = (\n",
    "    prompt\n",
    "    | chat_model.bind(function_call={\"name\": \"soccerfacts\"}, functions= functions)\n",
    "    | JsonOutputFunctionsParser()\n",
    ")\n",
    "\n",
    "\n",
    "chain.invoke({\"soccer_player\": \"Mbappe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_input': 'whatever', 'uppercase': 'WHATEVER'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_uppercase(arg):\n",
    "    return arg[\"original_input\"].upper()\n",
    "\n",
    "chain = RunnableParallel({\"original_input\": RunnablePassthrough()}).assign(uppercase=RunnableLambda(make_uppercase))\n",
    "chain.invoke(\"whatever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AYOUB'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough() | make_uppercase\n",
    "    )\n",
    "chain.invoke({\"original_input\":\"ayoub\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain inside chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Vous pensez à la France, et la France est située sur le continent européen.\\n\\n(Translation: You're thinking of France, and France is located on the European continent.)\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"what is the country {politician} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what continent is the country {country} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "chain1 = prompt1 | chat_model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"country\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"politician\": \"Miterrand\", \"language\": \"French\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fallback for chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(model=\"llama3-70b-8192\")\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "splits = text_spliter.split_documents(docs)\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\":retriever | RunnableLambda(format_docs) ,\"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is the process of breaking down a complex task into smaller, more manageable steps. It involves decomposing hard tasks into simpler steps, making it easier for an agent or model to plan and execute the task. This can be done through various methods, including Chain of Thought, Tree of Thoughts, and simple prompting or task-specific instructions.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_15756\\1046716321.py:14: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  conversation = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=chat_model,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_15756\\533826746.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  conversation({\"question\": \"My name is Julio and I have moved 33 times.\"})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: My name is Julio and I have moved 33 times.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'My name is Julio and I have moved 33 times.',\n",
       " 'chat_history': [HumanMessage(content='My name is Julio and I have moved 33 times.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Wow, Julio! 33 times is a lot of moving! I can only imagine how many different places you've lived in and the experiences you've had. That's incredible!\\n\\nWhat's been the most challenging part of moving so many times for you? Was it adjusting to new schools, making new friends, or something else entirely?\\n\\nAnd if you don't mind me asking, what's been the reason behind all these moves? Was it for work, family, or something else?\", additional_kwargs={}, response_metadata={})],\n",
       " 'text': \"Wow, Julio! 33 times is a lot of moving! I can only imagine how many different places you've lived in and the experiences you've had. That's incredible!\\n\\nWhat's been the most challenging part of moving so many times for you? Was it adjusting to new schools, making new friends, or something else entirely?\\n\\nAnd if you don't mind me asking, what's been the reason behind all these moves? Was it for work, family, or something else?\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation({\"question\": \"My name is Julio and I have moved 33 times.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're an assistant who's good at {ability}. Respond in 20 words or fewer\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "runnable = prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine is the ratio of the adjacent side to the hypotenuse in a right-angled triangle.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 37, 'total_tokens': 59, 'completion_time': 0.069623428, 'prompt_time': 0.001151258, 'queue_time': 0.014330572, 'total_time': 0.070774686}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_c1a4bcec29', 'finish_reason': 'stop', 'logprobs': None}, id='run-2f560d2e-5705-4615-8c51-7fe562e14956-0', usage_metadata={'input_tokens': 37, 'output_tokens': 22, 'total_tokens': 59})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine = adjacent side / hypotenuse.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 70, 'total_tokens': 81, 'completion_time': 0.034728488, 'prompt_time': 0.003179872, 'queue_time': 0.012308707, 'total_time': 0.03790836}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_c1a4bcec29', 'finish_reason': 'stop', 'logprobs': None}, id='run-df29bb32-8c93-48c6-90e9-818a35dcee7f-0', usage_metadata={'input_tokens': 70, 'output_tokens': 11, 'total_tokens': 81})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm here to help with math problems! What's the question or problem you need assistance with?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 34, 'total_tokens': 55, 'completion_time': 0.06, 'prompt_time': 0.004369841, 'queue_time': 0.009173239, 'total_time': 0.064369841}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_87cbfbbc4d', 'finish_reason': 'stop', 'logprobs': None}, id='run-fd6e63bc-244e-4824-b936-5d40d1ff0da7-0', usage_metadata={'input_tokens': 34, 'output_tokens': 21, 'total_tokens': 55})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New session_id --> does not remember.\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"def234\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
